{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "\n",
    "# Optimizing Computer Vision Applications\n",
    "\n",
    "This tutorial shows some techniques to get better performance for computer vision applications with the Intel® Distribution of OpenVINO™ toolkit.\n",
    "### 1. Tune parameters - set batch size\n",
    "\n",
    "In this section, we will see how changes in the batch size affect the performance. We will use the SSD300 model for the experiments.\n",
    "\n",
    "The default batch size for the Model Optimizer is 1.\n",
    "#### Let us first look at the performance numbers for the batch size 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[setupvars.sh] OpenVINO environment initialized\n",
      "[ INFO ] Initializing plugin for CPU device...\n",
      "[ INFO ] Reading IR...\n",
      "[ INFO ] Loading IR to the plugin...\n",
      "inputdims= 300 300 3 1\n",
      "outputdims= 7 100 1 1\n",
      "SSD Mode\n",
      "[ INFO ] Starting inference in async mode...\n",
      "framenum:256\n",
      "\n",
      "Preprocess:  10.277553461492062 \tms/frame\n",
      "Inference:   13.56345321983099 \tms/frame\n",
      "Postprocess: 0.8453469332090148 \tms/frame\n"
     ]
    }
   ],
   "source": [
    "!/opt/intel/openvino/bin/setupvars.sh\n",
    "! python3 /opt/intel/workshop/smart-video-workshop/object-detection/tutorial1.py -i /opt/intel/workshop/smart-video-workshop/object-detection/Cars\\ -\\ 1900.mp4 -m /opt/intel/workshop/smart-video-workshop/object-detection/mobilenet-ssd/FP32/mobilenet-ssd.xml -l /home/intel/inference_engine_samples_build/intel64/Release/lib/libcpu_extension.so "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "#### Change the batch size to 2 and run the object-detection example for new batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] Initializing plugin for CPU device...\n",
      "[ INFO ] Reading IR...\n",
      "[ INFO ] Loading IR to the plugin...\n",
      "inputdims= 300 300 3 1\n",
      "outputdims= 7 100 1 1\n",
      "SSD Mode\n",
      "[ INFO ] Starting inference in async mode...\n",
      "framenum:256\n",
      "\n",
      "Preprocess:  10.16246434301138 \tms/frame\n",
      "Inference:   3.913639113306999 \tms/frame\n",
      "Postprocess: 0.4350638111277777 \tms/frame\n"
     ]
    }
   ],
   "source": [
    "! python3 /opt/intel/workshop/smart-video-workshop/object-detection/tutorial1.py -i /opt/intel/workshop/smart-video-workshop/object-detection/Cars\\ -\\ 1900.mp4 -m /opt/intel/workshop/smart-video-workshop/object-detection/mobilenet-ssd/FP32/mobilenet-ssd.xml -l /home/intel/inference_engine_samples_build/intel64/Release/lib/libcpu_extension.so  -b 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "#### Run the example for different batch sizes\n",
    "\n",
    "Change the batch sizes to 8,16,32,64,128 and so on and see the performance diffrence in terms of the inference time.\n",
    "### 2. Pick the right model based on application and hardware\n",
    "\n",
    "Use/train a model with the right performance/accuracy tradeoffs. Performance differences between models can be bigger than any optimization you can do at the inference app level. Run various SSD models from the model_downloader in the car detection example which we used in the initial tutorial and observe the performance. We will run these tests on different hardware accelerators to determine how application performance depends on models as well as hardware.\n",
    "#### Run Model Optimizer on the models to get IR files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /opt/intel/workshop/smart-video-workshop/object-detection/SSD512/{FP16,FP32} \n",
    "!mkdir -p /opt/intel/workshop/smart-video-workshop/object-detection/SSD300/{FP16,FP32} \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/opt/intel/openvino/deployment_tools/tools/model_downloader/object_detection/common/ssd/512/caffe/ssd512.caffemodel\n",
      "\t- Path for generated IR: \t/opt/intel/workshop/smart-video-workshop/object-detection/SSD512/FP32\n",
      "\t- IR output name: \tssd512\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "Caffe specific parameters:\n",
      "\t- Enable resnet optimization: \tTrue\n",
      "\t- Path to the Input prototxt: \t/opt/intel/openvino/deployment_tools/tools/model_downloader/object_detection/common/ssd/512/caffe/ssd512.prototxt\n",
      "\t- Path to CustomLayersMapping.xml: \tDefault\n",
      "\t- Path to a mean file: \tNot specified\n",
      "\t- Offsets for a mean file: \tNot specified\n",
      "Model Optimizer version: \t2019.1.0-341-gc9b66a2\n",
      "\n",
      "[ SUCCESS ] Generated IR model.\n",
      "[ SUCCESS ] XML file: /opt/intel/workshop/smart-video-workshop/object-detection/SSD512/FP32/ssd512.xml\n",
      "[ SUCCESS ] BIN file: /opt/intel/workshop/smart-video-workshop/object-detection/SSD512/FP32/ssd512.bin\n",
      "[ SUCCESS ] Total execution time: 9.82 seconds. \n"
     ]
    }
   ],
   "source": [
    "! python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_caffe.py --input_model /opt/intel/openvino/deployment_tools/tools/model_downloader/object_detection/common/ssd/512/caffe/ssd512.caffemodel -o /opt/intel/workshop/smart-video-workshop/object-detection/SSD512/FP32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/opt/intel/openvino/deployment_tools/tools/model_downloader/object_detection/common/ssd/512/caffe/ssd512.caffemodel\n",
      "\t- Path for generated IR: \t/opt/intel/workshop/smart-video-workshop/object-detection/SSD512/FP16\n",
      "\t- IR output name: \tssd512\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tNot specified, inherited from the model\n",
      "\t- Output layers: \tNot specified, inherited from the model\n",
      "\t- Input shapes: \tNot specified, inherited from the model\n",
      "\t- Mean values: \tNot specified\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP16\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tFalse\n",
      "\t- Reverse input channels: \tFalse\n",
      "Caffe specific parameters:\n",
      "\t- Enable resnet optimization: \tTrue\n",
      "\t- Path to the Input prototxt: \t/opt/intel/openvino/deployment_tools/tools/model_downloader/object_detection/common/ssd/512/caffe/ssd512.prototxt\n",
      "\t- Path to CustomLayersMapping.xml: \tDefault\n",
      "\t- Path to a mean file: \tNot specified\n",
      "\t- Offsets for a mean file: \tNot specified\n",
      "Model Optimizer version: \t2019.1.0-341-gc9b66a2\n",
      "\n",
      "[ SUCCESS ] Generated IR model.\n",
      "[ SUCCESS ] XML file: /opt/intel/workshop/smart-video-workshop/object-detection/SSD512/FP16/ssd512.xml\n",
      "[ SUCCESS ] BIN file: /opt/intel/workshop/smart-video-workshop/object-detection/SSD512/FP16/ssd512.bin\n",
      "[ SUCCESS ] Total execution time: 9.87 seconds. \n"
     ]
    }
   ],
   "source": [
    "! python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_caffe.py --input_model /opt/intel/openvino/deployment_tools/tools/model_downloader/object_detection/common/ssd/512/caffe/ssd512.caffemodel -o /opt/intel/workshop/smart-video-workshop/object-detection/SSD512/FP16 --data_type FP16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Optimizer arguments:\r\n",
      "Common parameters:\r\n",
      "\t- Path to the Input Model: \t/opt/intel/openvino/deployment_tools/tools/model_downloader/object_detection/common/ssd/300/caffe/ssd300.caffemodel\r\n",
      "\t- Path for generated IR: \t/opt/intel/workshop/smart-video-workshop/object-detection/SSD300/FP32\r\n",
      "\t- IR output name: \tssd300\r\n",
      "\t- Log level: \tERROR\r\n",
      "\t- Batch: \tNot specified, inherited from the model\r\n",
      "\t- Input layers: \tNot specified, inherited from the model\r\n",
      "\t- Output layers: \tNot specified, inherited from the model\r\n",
      "\t- Input shapes: \tNot specified, inherited from the model\r\n",
      "\t- Mean values: \tNot specified\r\n",
      "\t- Scale values: \tNot specified\r\n",
      "\t- Scale factor: \tNot specified\r\n",
      "\t- Precision of IR: \tFP32\r\n",
      "\t- Enable fusing: \tTrue\r\n",
      "\t- Enable grouped convolutions fusing: \tTrue\r\n",
      "\t- Move mean values to preprocess section: \tFalse\r\n",
      "\t- Reverse input channels: \tFalse\r\n",
      "Caffe specific parameters:\r\n",
      "\t- Enable resnet optimization: \tTrue\r\n",
      "\t- Path to the Input prototxt: \t/opt/intel/openvino/deployment_tools/tools/model_downloader/object_detection/common/ssd/300/caffe/ssd300.prototxt\r\n",
      "\t- Path to CustomLayersMapping.xml: \tDefault\r\n",
      "\t- Path to a mean file: \tNot specified\r\n",
      "\t- Offsets for a mean file: \tNot specified\r\n",
      "Model Optimizer version: \t2019.1.0-341-gc9b66a2\r\n"
     ]
    }
   ],
   "source": [
    "! python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_caffe.py --input_model /opt/intel/openvino/deployment_tools/tools/model_downloader/object_detection/common/ssd/300/caffe/ssd300.caffemodel -o /opt/intel/workshop/smart-video-workshop/object-detection/SSD300/FP32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_caffe.py --input_model /opt/intel/openvino/deployment_tools/tools/model_downloader/object_detection/common/ssd/300/caffe/ssd300.caffemodel -o /opt/intel/workshop/smart-video-workshop/object-detection/SSD300/FP16 --data_type FP16\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "#### Set environmental variables and navigate to object detection tutorial directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! /opt/intel/openvino/bin/setupvars.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "#### a) CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 /opt/intel/workshop/smart-video-workshop/object-detection/tutorial1.py -i /opt/intel/workshop/smart-video-workshop/object-detection/Cars\\ -\\ 1900.mp4 -m /opt/intel/workshop/smart-video-workshop/object-detection/mobilenet-ssd/FP32/mobilenet-ssd.xml -l /home/intel/inference_engine_samples_build/intel64/Release/lib/libcpu_extension.so "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 /opt/intel/workshop/smart-video-workshop/object-detection/tutorial1.py -i /opt/intel/workshop/smart-video-workshop/object-detection/Cars\\ -\\ 1900.mp4 -m /opt/intel/workshop/smart-video-workshop/object-detection/SSD300/FP32/ssd300.xml -l /home/intel/inference_engine_samples_build/intel64/Release/lib/libcpu_extension.so \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 /opt/intel/workshop/smart-video-workshop/object-detection/tutorial1.py -i /opt/intel/workshop/smart-video-workshop/object-detection/Cars\\ -\\ 1900.mp4 -m /opt/intel/workshop/smart-video-workshop/object-detection/SSD512/FP32/ssd512.xml -l /home/intel/inference_engine_samples_build/intel64/Release/lib/libcpu_extension.so \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "#### b) GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 /opt/intel/workshop/smart-video-workshop/object-detection/tutorial1.py -i /opt/intel/workshop/smart-video-workshop/object-detection/Cars\\ -\\ 1900.mp4 -m /opt/intel/workshop/smart-video-workshop/object-detection/mobilenet-ssd/FP32/mobilenet-ssd.xml -d GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 /opt/intel/workshop/smart-video-workshop/object-detection/tutorial1.py -i /opt/intel/workshop/smart-video-workshop/object-detection/Cars\\ -\\ 1900.mp4 -m /opt/intel/workshop/smart-video-workshop/object-detection/SSD300/FP32/ssd300.xml -d GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 /opt/intel/workshop/smart-video-workshop/object-detection/tutorial1.py -i /opt/intel/workshop/smart-video-workshop/object-detection/Cars\\ -\\ 1900.mp4 -m /opt/intel/workshop/smart-video-workshop/object-detection/SSD512/FP32/ssd512.xml -d GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "#### c) Intel® Movidius™ Neural Compute Stick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 /opt/intel/workshop/smart-video-workshop/object-detection/tutorial1.py -i /opt/intel/workshop/smart-video-workshop/object-detection/Cars\\ -\\ 1900.mp4 -m /opt/intel/workshop/smart-video-workshop/object-detection/mobilenet-ssd/FP16/mobilenet-ssd.xml -d MYRIAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 /opt/intel/workshop/smart-video-workshop/object-detection/tutorial1.py -i /opt/intel/workshop/smart-video-workshop/object-detection/Cars\\ -\\ 1900.mp4 -m /opt/intel/workshop/smart-video-workshop/object-detection/SSD300/FP16/ssd300.xml -d MYRIAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 /opt/intel/workshop/smart-video-workshop/object-detection/tutorial1.py -i /opt/intel/workshop/smart-video-workshop/object-detection/Cars\\ -\\ 1900.mp4 -m /opt/intel/workshop/smart-video-workshop/object-detection/SSD512/FP16/ssd512.xml -d MYRIAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "Note: There is often USB write error for Intel® Movidius™ Neural Compute Stick, please try re-running the command. Sometimes it takes 3 trials.\n",
    "\n",
    "#### 3. Use the right data type for your target harware and accuracy needs\n",
    "\n",
    "In this section, we will consider an example running on a GPU. FP16 operations are better optimized than FP32 on GPUs. We will run the object detection example with SSD models with data types FP16 and FP32 and observe the performance difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 /opt/intel/workshop/smart-video-workshop/object-detection/tutorial1.py -i /opt/intel/workshop/smart-video-workshop/object-detection/Cars\\ -\\ 1900.mp4 -m /opt/intel/workshop/smart-video-workshop/object-detection/SSD300/FP32/ssd300.xml -d GPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 /opt/intel/workshop/smart-video-workshop/object-detection/tutorial1.py -i /opt/intel/workshop/smart-video-workshop/object-detection/Cars\\ -\\ 1900.mp4 -m /opt/intel/workshop/smart-video-workshop/object-detection/SSD300/FP16/ssd300.xml -d GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "It is clear that we got better performance with FP16 models.\n",
    "#### 4. Use async\n",
    "\n",
    "The async API can improve the overall frame rate of the application. While the accelerator is busy with running inference operations, the application can continue encoding, decoding or post inference data processing on the host. For this section, we will use the object_detection_demo_ssd_async sample. This sample makes asynchronous requests to the inference engine. This reduces the inference request latency, so that the overall framerate is determined by the MAXIMUM(detection time, input capturing time) and not the SUM(detection time, input capturing time).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "    \n",
    "#### a) Run the async example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! python3 /opt/intel/openvino/deployment_tools/inference_engine/samples/python_samples/object_detection_demo_ssd_async/object_detection_demo_ssd_async.py -i /opt/intel/workshop/smart-video-workshop/object-detection/Cars\\ -\\ 1900.mp4 -m /opt/intel/workshop/smart-video-workshop/object-detection/mobilenet-ssd/FP32/mobilenet-ssd.xml -l /home/intel/inference_engine_samples_build/intel64/Release/lib/libcpu_extension.so "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 19px\">\n",
    "\n",
    "- Press tab to switch to sync mode. Observe the number of fps (frames per second) for both sync and async mode. The number frames processed per second are more in async than the sync mode.\n",
    "\n",
    "There are important performance caveats though. Tasks that run in parallel should try to avoid oversubscribing to shared computing resources. For example, if the inference tasks are running on the FPGA and the CPU is essentially idle, then it makes sense to run tasks on the CPU in parallel."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
